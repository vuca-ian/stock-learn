{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.plotting import table\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from torchsummary import summary\n",
    "from torchinfo import summary\n",
    "\n",
    "from typing import Dict, Any\n",
    "import platform\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.family'] = ['Songti SC']\n",
    "elif platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = ['SimSun']\n",
    "else:  # Linux\n",
    "    plt.rcParams['font.family'] = ['Noto Sans CJK SC']\n",
    "# matplotlib.rcParams['font.family']= ['Songti SC']  # 使用黑体-简\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "with open('./config.json', 'r', encoding='utf-8') as f:\n",
    "    config= json.load(f)\n",
    "\n",
    "print(json.dumps(config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleStockDataset(Dataset):\n",
    "    \"\"\"多模尺度数据集\"\"\"\n",
    "    def __init__(self, scale_factors, seq_length=24, pred_length=4, data_path='./'):\n",
    "        self.standardScaler = StandardScaler()\n",
    "        self.scale_factors = scale_factors\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "        self.data_path = data_path\n",
    "        self.data = {\n",
    "            scale: self._generate_synthetic_data(scale) for scale in self.scale_factors\n",
    "        }\n",
    "        t30 = pd.read_csv(self.data_path + 'train-30min.csv', \n",
    "                           parse_dates=['date'], index_col='date')\n",
    "        self.feature_names = t30.columns.tolist()  # 新增特征名称存储\n",
    "        self.feature_dims = {\n",
    "            scale: data.shape[1] for scale, data in self.data.items()\n",
    "        }\n",
    "        # print(t30.tail())\n",
    "        # 验证数据维度\n",
    "        for scale, data in self.data.items():\n",
    "            min_required = seq_length * {'30min':48, '1hour':24, '4hour':6, '1day':1}[scale]\n",
    "            assert len(data) >= min_required, \\\n",
    "                f\"{scale}数据至少需要{min_required}条，当前仅{len(data)}条\"\n",
    "            # assert len(data) > seq_length + pred_length, \\\n",
    "            #     f\"{scale}数据长度不足，需要至少{seq_length + pred_length}个样本\"\n",
    "            # assert data.shape[1] == 4, \\\n",
    "            #     f\"{scale}数据特征维度应为4，实际为{data.shape[1]}\"\n",
    "    \n",
    "    def _generate_synthetic_data(self, scale):\n",
    "        \"\"\"获取合成数据\"\"\"\n",
    "        data = pd.read_csv(self.data_path + f'train-{scale}.csv', \n",
    "                           usecols=['date','open', 'high', 'low', 'close'],\n",
    "                           parse_dates=['date'], index_col='date')\n",
    "        return self.standardScaler.fit_transform(data)\n",
    "        # return data\n",
    "    \n",
    "    def _align_scales(self, idx):\n",
    "        \"\"\"三线性插值对齐时间轴\"\"\"\n",
    "        aligned = {}\n",
    "        base_idx = idx // 8  # 对齐到日线级别\n",
    "        for scale in self.scale_factors:\n",
    "            ratio = {'30min':48, '1hour':24, '4hour':6, '1day':1}[scale]\n",
    "\n",
    "            scale_idx = int(base_idx * ratio)\n",
    "            seq_items = int(self.seq_length * ratio)\n",
    "\n",
    "            max_valid_idx = len(self.data[scale]) - seq_items\n",
    "            scale_idx = max(0, min(scale_idx, max_valid_idx))  # 确保不越界\n",
    "\n",
    "            aligned_seq = self.data[scale][scale_idx:scale_idx + seq_items]\n",
    "            # 添加空数据检查\n",
    "            if len(aligned_seq) == 0:\n",
    "                aligned_seq = np.zeros((self.seq_length, 4))  # 用零填充\n",
    "            if isinstance(aligned_seq, pd.DataFrame):\n",
    "                aligned_seq = aligned_seq.values\n",
    "            # 确保转换为numpy数组\n",
    "            if isinstance(aligned_seq, np.ndarray):\n",
    "                aligned_seq = aligned_seq.copy()\n",
    "            else:\n",
    "                aligned_seq = aligned_seq.values.copy()\n",
    "            # Linear interpolation if needed\n",
    "            if len(aligned_seq) < self.seq_length:\n",
    "                x_orig = np.arange(len(aligned_seq))\n",
    "                x_new = np.linspace(0, len(aligned_seq)-1, self.seq_length)\n",
    "                aligned_seq = np.array([np.interp(x_new, x_orig, col) \n",
    "                                    for col in aligned_seq.T]).T\n",
    "                \n",
    "            aligned[scale] = aligned_seq[:self.seq_length]\n",
    "            # 修改对齐处理（如果需要保持31维）\n",
    "            # aligned = {k: np.pad(v, ((0,0),(0,27))) if v.shape[1]==4 else v  # 4 → 31\n",
    "            #         for k,v in inputs.items()}\n",
    "        return aligned\n",
    "    # 新增方法：可视化对齐后的样本特征\n",
    "    def visualize_aligned_features(self, num_samples=3):\n",
    "        \"\"\"可视化对齐后的特征结构\"\"\"\n",
    "        for _ in range(num_samples):\n",
    "            idx = np.random.randint(0, len(self))\n",
    "            inputs, target = self[idx]\n",
    "            \n",
    "            fig, axs = plt.subplots(len(self.scale_factors), 1, \n",
    "                                  figsize=(12, 2*len(self.scale_factors)))\n",
    "            plt.suptitle(f\"样本 {idx} 对齐特征可视化\", y=1.02)\n",
    "            \n",
    "            for i, (scale, tensor) in enumerate(inputs.items()):\n",
    "                data = tensor.numpy()\n",
    "                ax = axs[i] if len(self.scale_factors) > 1 else axs\n",
    "                for j in range(data.shape[1]):\n",
    "                    ax.plot(data[:, j], label=f'{self.feature_names[j]}')\n",
    "                ax.set_title(f\"{scale}尺度特征序列\")\n",
    "                ax.legend(loc='upper right')\n",
    "                ax.grid(True)\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 打印目标值信息\n",
    "            print(f\"\\n目标值（{self.pred_length}步后预测）: {target.item():.4f}\")\n",
    "            print(\"-\"*50)\n",
    "    def show_aligned_samples(self, num_samples=3):\n",
    "        \"\"\"可视化随机样本的对齐结果\"\"\"\n",
    "        for _ in range(num_samples):\n",
    "            idx = np.random.randint(0, len(self))\n",
    "            aligned = self._align_scales(idx)\n",
    "            # 创建带表格的可视化布局\n",
    "            fig = plt.figure(figsize=(16, 12))\n",
    "            \n",
    "            # 第一部分：原始数据对比\n",
    "            for i, scale in enumerate(self.scale_factors, 1):\n",
    "                # 获取原始数据片段\n",
    "                raw_data = self.data[scale][idx:idx+self.seq_length]\n",
    "                if isinstance(raw_data, pd.DataFrame):\n",
    "                    raw_data = raw_data.values  # 转换为numpy数组\n",
    "                assert len(raw_data) > 0, f\"{scale}数据长度不足，Dataset cannot be empty\"\n",
    "                # 创建双子图（曲线+表格）\n",
    "                ax1 = plt.subplot(len(self.scale_factors), 2, 2*i-1)\n",
    "                ax1.plot(raw_data[:, 0], 'g-', label='原始数据')\n",
    "                ax1.plot(aligned[scale][:, 0], 'b--', label='对齐数据')\n",
    "                ax1.set_title(f'{scale} 对齐对比')\n",
    "                \n",
    "                # 表格子图\n",
    "                ax2 = plt.subplot(len(self.scale_factors), 2, 2*i)\n",
    "                ax2.axis('off')\n",
    "                # 构建对比数据表\n",
    "                comparison_df = pd.DataFrame({\n",
    "                    '指标': ['均值', '标准差', '最大值', '最小值'],\n",
    "                    '原始数据': [\n",
    "                        raw_data[:,0].mean(),\n",
    "                        raw_data[:,0].std(),\n",
    "                        raw_data[:,0].max(),\n",
    "                        raw_data[:,0].min()\n",
    "                    ],\n",
    "                    '对齐数据': [\n",
    "                        aligned[scale][:,0].mean(),\n",
    "                        aligned[scale][:,0].std(),\n",
    "                        aligned[scale][:,0].max(),\n",
    "                        aligned[scale][:,0].min()\n",
    "                    ]\n",
    "                })\n",
    "                comparison_df.columns = ['指标', '原始数据', '对齐数据']\n",
    "                ax2.table(cellText=comparison_df.values,\n",
    "                        colLabels=comparison_df.columns,\n",
    "                        loc='center',\n",
    "                        cellLoc='center',\n",
    "                        colColours=['#f0f0f0']*3)  # 添加列颜色\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self._align_scales(idx)\n",
    "\n",
    "        # #  # 确保填充后的维度一致性\n",
    "        # # for k,v in inputs.items():\n",
    "        # #     assert v.shape[1] == 4, f\"特征维度错误: {k}的维度为{v.shape[1]}\"\n",
    "\n",
    "        #  # 统一所有尺度特征维度为4\n",
    "        # aligned = {k: np.pad(v, ((0,0),(0,1))) if v.shape[1]==3 else v \n",
    "        #       for k,v in inputs.items()}\n",
    "        base_idx = idx + self.seq_length  # 确保索引不超过数据长度\n",
    "        target = self.data['30min'][base_idx + self.pred_length - 1, 0]  # 从[4]变为标量\n",
    "\n",
    "        # # 添加安全索引检查\n",
    "        # try:\n",
    "        #     inputs = self._align_scales(idx)\n",
    "        # except IndexError as e:\n",
    "        #     print(f\"索引错误 idx={idx}, 数据长度: {len(self.data['30min'])}\")\n",
    "        #     raise e\n",
    "        # # 双重验证\n",
    "        # assert isinstance(target, np.float64), \"目标值类型错误\"\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "        aligned = inputs.copy()  # 直接使用原始对齐结果\n",
    "        return {k:torch.FloatTensor(v) for k,v in aligned.items()}, target_tensor\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data[self.scale_factors[0]]) - self.pred_length - self.seq_length)\n",
    "\n",
    "class ScaleAwareExpert(nn.Module):\n",
    "    \"\"\"专家\"\"\"\n",
    "    def __init__(self, input_dim, scale_type):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scale_type = scale_type\n",
    "        self.input_dim = input_dim  # 动态传入特征维度\n",
    "        if '30min' in scale_type or '1h' in scale_type:\n",
    "            \"\"\"高频专家: CNN + BiLSTM \"\"\"\n",
    "            self.conv = nn.Conv1d(input_dim,\n",
    "                                  out_channels=max(16, input_dim//4),\n",
    "                                  kernel_size=3, \n",
    "                                  padding='same', \n",
    "                                  )\n",
    "            self.lstm = nn.LSTM(input_size= max(16, input_dim//4),\n",
    "                                hidden_size= input_dim*4, \n",
    "                                num_layers=2, \n",
    "                                batch_first=True,\n",
    "                                bidirectional=True)\n",
    "            self.proj = nn.Linear(input_dim*8,input_dim*2)\n",
    "        else:\n",
    "            \"\"\"低频专家: LSTM \"\"\"\n",
    "            self.hidden_dim = max(4, input_dim // 4)  # 统一隐藏层维度\n",
    "            assert self.hidden_dim >= 4, \"特征维度过小\"\n",
    "            self.proj_in = nn.Linear(input_dim, self.hidden_dim)  \n",
    "            # 确保nhead能整除d_model\n",
    "            self.nhead = max(2, self.hidden_dim // 8)\n",
    "            while self.hidden_dim % self.nhead != 0:  # 自动适配可整除的nhead\n",
    "                self.nhead -= 1\n",
    "            self.nhead = max(1, self.nhead)  # 至少保留1个头\n",
    "            self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_dim,\n",
    "                                                            nhead=self.nhead,\n",
    "                                                            dim_feedforward=self.hidden_dim*4, \n",
    "                                                            batch_first=True)\n",
    "            self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n",
    "            self.proj = nn.Linear(self.hidden_dim, 32)\n",
    "        self.predictor = nn.Linear(32 if 'day' in scale_type else input_dim*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if '30min' in self.scale_type or '1h' in self.scale_type:\n",
    "            x = self.conv(x.permute(0,2,1)).permute(0,2,1)\n",
    "            x, _ = self.lstm(x)\n",
    "            x = x[:, -1, :]  # 取最后一个时间步 [batch, 4]\n",
    "            x = self.proj(x)\n",
    "        else:\n",
    "            x = self.proj_in(x)\n",
    "            # 添加维度校验\n",
    "            if x.size(-1) != self.hidden_dim:\n",
    "                raise ValueError(f\"Transformer输入维度错误，期望{self.hidden_dim}，实际{x.size(-1)}\")\n",
    "                \n",
    "            x = self.transformer(x)\n",
    "            x = self.proj(x[:,-1,:]) # 只取最后一个时间步的输出\n",
    "        return self.predictor(x)\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"混合专家网络\"\"\"\n",
    "    def __init__(self, experts=['30min','1hour','4hour','1day'], feature_dims=None):\n",
    "        super().__init__()\n",
    "        # 动态计算门控网络输入维度\n",
    "        total_gate_dim = sum(feature_dims[scale] for scale in experts)\n",
    "        self.experts = nn.ModuleDict({\n",
    "            scale: ScaleAwareExpert(input_dim=feature_dims[scale], scale_type=scale) for scale in experts\n",
    "        })\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(total_gate_dim, total_gate_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(total_gate_dim//2, len(experts)),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        # Add attention-based gate mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=32, num_heads=4)\n",
    "        # Add auxiliary outputs for interpretability\n",
    "        self.auxiliary = nn.ModuleDict({\n",
    "            'volatility': nn.Linear(32, 1),\n",
    "            'trend': nn.Linear(32, 1)\n",
    "        })\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        expert_outputs = {}\n",
    "        for scale, expert in self.experts.items():\n",
    "            expert_outputs[scale] = expert(inputs[scale]).squeeze(-1)  # [32,1] -> [32]\n",
    "        # 动态门控\n",
    "        gate_input = torch.cat([v for v in inputs.values()], dim=-1)\n",
    "        weights = self.gate(gate_input.mean(dim=1))  # (batch, num_experts)\n",
    "        \n",
    "        # 加权融合\n",
    "        combined = sum(weights[:, i] * expert_outputs[scale] \n",
    "                      for i, scale in enumerate(self.experts.keys()))\n",
    "        return combined.unsqueeze(-1), expert_outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"训练模型\"\"\"\n",
    "def train(dataset, model_config: Dict[str, Dict[str, Any]], trade_config: Dict[str, Dict[str, Any]],scale_factors=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    loader = DataLoader(dataset, batch_size=model_config['batch_size'], shuffle=True)\n",
    "    \n",
    "    model = MoE(experts=scale_factors, feature_dims=dataset.feature_dims).to(device)\n",
    "    for name, expert in model.experts.items():\n",
    "        print(f\"{name}专家输入维度: {expert.input_dim}\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    # print_model_summary(model, device)\n",
    "    # 添加可视化数据收集\n",
    "    # ==================== 训练逻辑 ====================\n",
    "    def hybrid_loss(pred, target, expert_outs, alpha=0.7):\n",
    "        pred = pred.squeeze(-1)  # (32,1) → (32)\n",
    "        target = target.squeeze()  # [32] remains\n",
    "        mse = nn.MSELoss()\n",
    "        main_loss = mse(pred, target)\n",
    "        \n",
    "        # 趋势一致性约束\n",
    "        trends = torch.stack([torch.sign(out.detach()) for out in expert_outs.values()])\n",
    "        consistency = torch.mean(torch.prod(trends, dim=0))\n",
    "        \n",
    "        return alpha*main_loss + (1-alpha)*(1 - consistency)\n",
    "    loss_history = []\n",
    "    expert_weights = []\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for epoch in range(trade_config['num_epochs']):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, target) in enumerate(loader):\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "            target = target.to(device)\n",
    "            \n",
    "            pred, expert_outs = model(inputs)\n",
    "            loss = hybrid_loss(pred, target, expert_outs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # 收集预测结果\n",
    "            with torch.no_grad():\n",
    "                preds = pred.squeeze().cpu().numpy()\n",
    "                predictions.extend(preds)\n",
    "                targets.extend(target.cpu().numpy())\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        # 记录专家权重\n",
    "        with torch.no_grad():\n",
    "            dummy_input = {k: torch.randn(1,24,4).to(device) for k in model.experts}\n",
    "            _, weights = model(dummy_input)\n",
    "            expert_weights.append([list(out.parameters())[0].detach().cpu().numpy().mean() for out in model.experts.values()])\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        loss_history.append(avg_loss)    \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch} Loss: {loss.item():.4f}')\n",
    "   \n",
    "    return model, (loss_history, expert_weights), (targets, predictions)\n",
    "dataset = MultiScaleStockDataset(data_path=config['data_path'], scale_factors=config['scale_factors'])\n",
    "print(f\"特征数量: {len(dataset.feature_names)}\")  # 应该输出4\n",
    "print(f\"特征维度分布:{dataset.feature_dims}\")\n",
    "print(f\"样本维度: {dataset.data['30min'].shape}\")  # 应该为 (N,4)\n",
    "# dataset.visualize_aligned_features(num_samples=2)\n",
    "\n",
    "# 查看对齐对比（含特征名称）\n",
    "# dataset.show_aligned_samples(num_samples=3)\n",
    "model, history, results  = train(dataset, config['props'][0]['model_config'],config['props'][0]['trade_config'], config['scale_factors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(loss_history, expert_weights, targets, predictions):\n",
    "    \"\"\"训练过程可视化分析\"\"\"\n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    # 损失曲线\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(loss_history, label='Training Loss')\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\"), plt.ylabel(\"Loss\")\n",
    "    \n",
    "    # 专家权重分布\n",
    "    plt.subplot(1,3,2)\n",
    "    sns.heatmap(np.array(expert_weights), cmap='viridis', \n",
    "                xticklabels=['30min','1h','4h','1d'])\n",
    "    plt.title(\"Expert Weights Distribution\")\n",
    "    plt.xlabel(\"Experts\"), plt.ylabel(\"Epoch\")\n",
    "    \n",
    "    # 预测结果示例\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.scatter(targets[:100], predictions[:100], alpha=0.5)\n",
    "    plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'r--')\n",
    "    plt.title(\"Predictions vs Ground Truth\")\n",
    "    plt.xlabel(\"True Values\"), plt.ylabel(\"Predictions\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def enhanced_visualization(loss_history, expert_weights, param_distributions, config):\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Loss curve\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.semilogy(loss_history)\n",
    "    plt.title(\"Training Loss (Log Scale)\")\n",
    "    \n",
    "    # Expert weights heatmap\n",
    "    plt.subplot(2,3,2)\n",
    "    sns.heatmap(np.array(expert_weights), cmap='viridis', \n",
    "                annot=True, fmt=\".2f\",\n",
    "                xticklabels=config[\"scale_factors\"])\n",
    "    plt.title(\"Expert Weight Distribution\")\n",
    "    \n",
    "    # Parameter distributions\n",
    "    plt.subplot(2,3,3)\n",
    "    for name, values in param_distributions.items():\n",
    "        sns.kdeplot(values, label=name)\n",
    "    plt.title(\"Parameter Value Distributions\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Gradient flow\n",
    "    plt.subplot(2,3,4)\n",
    "    for name, grads in param_distributions.items():\n",
    "        plt.plot(grads, label=name)\n",
    "    plt.title(\"Gradient Flow\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"可视化分析\"\"\"\n",
    "visualize_training(history[0], history[1], targets=results[0], predictions=results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_experts(model, device, test_loader):\n",
    "    \"\"\"各专家输出分析\"\"\"\n",
    "    expert_outputs = {name: [] for name in model.experts}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "            _, outputs = model(inputs)\n",
    "            \n",
    "            for name, out in outputs.items():\n",
    "                expert_outputs[name].extend(out.squeeze().cpu().numpy())\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    for name, values in expert_outputs.items():\n",
    "        sns.kdeplot(values, label=name, alpha=0.6)\n",
    "    plt.title(\"Expert Output Distributions\")\n",
    "    plt.xlabel(\"Prediction Value\"), plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n=== Expert Analysis ===\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_dataset = MultiScaleStockDataset(\n",
    "    data_path=config['data_path'], \n",
    "    scale_factors=config['scale_factors']\n",
    ")\n",
    "model_config = config['props'][0]['model_config']\n",
    "test_loader = DataLoader(test_dataset, batch_size=model_config['batch_size'], shuffle=False)\n",
    "analyze_experts(model,device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
