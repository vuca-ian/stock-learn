{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleStockDataset(Dataset):\n",
    "    \"\"\"多模尺度数据集\"\"\"\n",
    "    def __init__(self, scale_factors, seq_length=24, pred_length=4, data_path='./'):\n",
    "        self.standardScaler = StandardScaler()\n",
    "        self.scale_factors = scale_factors\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_length = pred_length\n",
    "        self.data_path = data_path\n",
    "        self.data = {\n",
    "            scale: self._generate_synthetic_data(scale) for scale in self.scale_factors\n",
    "        }\n",
    "    \n",
    "    def _generate_synthetic_data(self, scale):\n",
    "        \"\"\"获取合成数据\"\"\"\n",
    "        data = pd.read_csv(self.data_path + f'train-{scale}.csv', \n",
    "                           usecols=['date','open', 'high', 'low', 'close'],\n",
    "                           parse_dates=['date'], index_col='date')\n",
    "        return self.standardScaler.fit_transform(data)\n",
    "    \n",
    "    def _align_scales(self, idx):\n",
    "        \"\"\"三线性插值对齐时间轴\"\"\"\n",
    "        aligned = {}\n",
    "        base_idx = idx // 8  # 对齐到日线级别\n",
    "        for scale in self.scale_factors:\n",
    "            ratio = {'30min':8, '1hour':4, '4hour':1, '1day':1/24}[scale]\n",
    "            start = int(base_idx * ratio)\n",
    "            aligned[scale] = self.data[scale][start:start+self.seq_length]\n",
    "        return aligned\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self._align_scales(idx)\n",
    "         # 确保填充后的维度一致性\n",
    "        for k,v in inputs.items():\n",
    "            assert v.shape[1] == 4, f\"特征维度错误: {k}的维度为{v.shape[1]}\"\n",
    "\n",
    "        target = self.data['30min'][idx:idx+self.pred_length, 0]  # 预测未来价格\n",
    "         # 统一所有尺度特征维度为4\n",
    "        aligned = {k: np.pad(v, ((0,0),(0,1))) if v.shape[1]==3 else v \n",
    "              for k,v in inputs.items()}\n",
    "        return  {k:torch.FloatTensor(v) for k,v in aligned.items()}, target\n",
    "    def __len__(self):\n",
    "        return len(self.data[self.scale_factors[0]]) - self.pred_length - self.seq_length\n",
    "\n",
    "class ScaleAwareExpert(nn.Module):\n",
    "    \"\"\"专家\"\"\"\n",
    "    def __init__(self, input_dim, scale_type):\n",
    "        super().__init__()\n",
    "        self.scale_type = scale_type\n",
    "\n",
    "        if '30min' in scale_type or '1h' in scale_type:\n",
    "            \"\"\"高频专家: CNN + BiLSTM \"\"\"\n",
    "            self.conv = nn.Conv1d(input_dim,\n",
    "                                  out_channels=16,\n",
    "                                  kernel_size=3, \n",
    "                                  padding='same', \n",
    "                                  )\n",
    "            self.lstm = nn.LSTM(16,32, num_layers=2, batch_first=True, bidirectional=True)\n",
    "            self.proj = nn.Linear(64,32)\n",
    "        else:\n",
    "            \"\"\"低频专家: LSTM \"\"\"\n",
    "            self.proj_in = nn.Linear(input_dim, 4)  # 3维→4维\n",
    "            self.encoder_layer = nn.TransformerDecoderLayer(d_model=4, nhead=2, dim_feedforward=64)\n",
    "            self.transformer = nn.TransformerDecoder(self.encoder_layer, num_layers=3)\n",
    "            self.proj = nn.Linear(4, 32)\n",
    "        self.predictor = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"输入维度: {x.shape}\")  # 应显示(batch, seq_len, 4)\n",
    "        if '30min' in self.scale_type or '1h' in self.scale_type:\n",
    "            x = self.conv(x.permute(0,2,1)).permute(0,2,1)\n",
    "            x, _ = self.lstm(x)\n",
    "            x = self.proj(x)\n",
    "        else:\n",
    "            x = self.proj_in(x)\n",
    "            x = self.transformer(x)\n",
    "        x = self.proj(x[:,-1,:]) # 只取最后一个时间步的输出\n",
    "        return self.predictor(x)\n",
    "class MoE(nn.Module):\n",
    "    \"\"\"混合专家网络\"\"\"\n",
    "    def __init__(self, input_dim=4, experts=['30min','1hour','4hour','1day'], hidden_dim=64, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleDict({\n",
    "            scale: ScaleAwareExpert(input_dim, scale) for scale in experts\n",
    "        })\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_dim * len(experts), hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, len(experts)),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        expert_outputs = {}\n",
    "        for scale, expert in self.experts.items():\n",
    "            expert_outputs[scale] = expert(inputs[scale])\n",
    "        # 动态门控\n",
    "        gate_input = torch.cat([v for v in inputs.values()], dim=-1)\n",
    "        weights = self.gate(gate_input.mean(dim=1))  # (batch, num_experts)\n",
    "        \n",
    "        # 加权融合\n",
    "        combined = sum(weights[:, i] * expert_outputs[scale] \n",
    "                      for i, scale in enumerate(self.experts.keys()))\n",
    "        return combined, expert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 训练逻辑 ====================\n",
    "def hybrid_loss(pred, target, expert_outs, alpha=0.7):\n",
    "    mse = nn.MSELoss()\n",
    "    main_loss = mse(pred, target)\n",
    "    \n",
    "    # 趋势一致性约束\n",
    "    trends = torch.stack([torch.sign(out.detach()) for out in expert_outs.values()])\n",
    "    consistency = torch.mean(torch.prod(trends, dim=0))\n",
    "    \n",
    "    return alpha*main_loss + (1-alpha)*(1 - consistency)\n",
    "def train():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = MultiScaleStockDataset(data_path='../data/', scale_factors=['30min', '1hour', '4hour', '1day'])\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model = MoE(input_dim=3).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for batch_idx, (inputs, target) in enumerate(loader):\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "            target = target.to(device)\n",
    "            \n",
    "            pred, expert_outs = model(inputs)\n",
    "            loss = hybrid_loss(pred, target, expert_outs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch} Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 评估指标 ==================== \n",
    "def sharpe_ratio(returns, risk_free=0.0):\n",
    "    excess_returns = returns - risk_free\n",
    "    return excess_returns.mean() / excess_returns.std()\n",
    "\n",
    "def max_drawdown(returns):\n",
    "    cumulative = returns.cumsum()\n",
    "    peak = cumulative.expanding(min_periods=1).max()\n",
    "    drawdown = (peak - cumulative).max()\n",
    "    return drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度: torch.Size([2, 24, 4])\n",
      "测试失败: mat1 and mat2 shapes cannot be multiplied (2x32 and 64x32)\n"
     ]
    }
   ],
   "source": [
    "def test_conv_layer():\n",
    "    # 模拟输入 (batch=2, seq=24, features=4)\n",
    "    test_input = torch.randn(2, 24, 4)\n",
    "    expert = ScaleAwareExpert(input_dim=4, scale_type='30min')\n",
    "    \n",
    "    # 前向传播检查\n",
    "    try:\n",
    "        output = expert(test_input)\n",
    "        print(f\"测试通过，输出维度: {output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"测试失败: {str(e)}\")\n",
    "\n",
    "test_conv_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入维度: torch.Size([32, 24, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 3, 3], expected input[32, 4, 24] to have 3 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[76], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     24\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 26\u001b[0m pred, expert_outs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m hybrid_loss(pred, target, expert_outs)\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\liufa\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\liufa\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[75], line 95\u001b[0m, in \u001b[0;36mMoE.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     93\u001b[0m expert_outputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scale, expert \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 95\u001b[0m     expert_outputs[scale] \u001b[38;5;241m=\u001b[39m \u001b[43mexpert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 动态门控\u001b[39;00m\n\u001b[0;32m     97\u001b[0m gate_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\liufa\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\liufa\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[75], line 70\u001b[0m, in \u001b[0;36mScaleAwareExpert.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m输入维度: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 应显示(batch, seq_len, 4)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m30min\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1h\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_type:\n\u001b[1;32m---> 70\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     71\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n",
      "File \u001b[1;32mc:\\Users\\liufa\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\liufa\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\liufa\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\liufa\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 3, 3], expected input[32, 4, 24] to have 3 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
