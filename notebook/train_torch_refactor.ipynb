{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import platform\n",
    "import datetime\n",
    "import logging\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    plt.rcParams['font.family'] = ['Songti SC']\n",
    "elif platform.system() == 'Windows':\n",
    "    plt.rcParams['font.family'] = ['SimSun']\n",
    "else:  # Linux\n",
    "    plt.rcParams['font.family'] = ['Noto Sans CJK SC']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False # 解决负号显示问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "with open('./config.json', 'r', encoding='utf-8') as f:\n",
    "    config= json.load(f)\n",
    "\n",
    "print(json.dumps(config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiScaleStockDataset(Dataset):\n",
    "    \"\"\"\n",
    "    scaler_factors=['30min', '1hour', '4hour', '1day'],\n",
    "    \"\"\"\n",
    "    def __init__(self, scaler_factors=None, data=None,\n",
    "                 seq_length=120,\n",
    "                 pred_steps=1,\n",
    "                 target_scale='30min',\n",
    "                 file_path=None, prefix=None):\n",
    "        super().__init__()\n",
    "        if scaler_factors is None:\n",
    "            scaler_factors = ['30min', '1hour', '4hour', '1day']\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.pred_steps= pred_steps\n",
    "        self.target_scale = target_scale\n",
    "        self.scaler_factors = scaler_factors\n",
    "        self.normalized_data = {}  # 存储归一化后数据\n",
    "        self.feature_scaler = RobustScaler()\n",
    "        self.raw_data = {\n",
    "            scaler: self._import_synthetic_data(file_path=file_path, file_name=f\"{prefix}{scaler}\") for scaler in scaler_factors\n",
    "        }\n",
    "        self.time_serial = self.raw_data[scaler_factors[0]].index\n",
    "        self.aligned_data = self._align_time_index(data=self.raw_data)\n",
    "        self._normalize_data()\n",
    "        self.feature_dims = {\n",
    "            scale: data.shape[1] for scale, data in self.raw_data.items()\n",
    "        }\n",
    "        self.feature_names = self.raw_data[scaler_factors[0]].columns.tolist()\n",
    "\n",
    "    def split_dataset(self, test_ratio=0.2):\n",
    "        \"\"\"时序安全的数据分割\"\"\"\n",
    "        total_len = len(self)\n",
    "        split_idx = int(total_len * (1 - test_ratio))\n",
    "        return Subset(self, range(split_idx)), Subset(self, range(split_idx, total_len))\n",
    "\n",
    "    def _align_time_index(self, base_scale='30min', data=None):\n",
    "        \"\"\"以最细粒度时间轴为基准进行对齐\"\"\"\n",
    "        base_df = data[base_scale]\n",
    "        aligned_data = {}\n",
    "        daily_df = data['1day'].copy()    \n",
    "        # 将日级数据时间对齐到基准时间轴的自然日边界\n",
    "        daily_df.index = daily_df.index.normalize() + pd.Timedelta(hours=23)  # 对齐到23:00  \n",
    "        for scale, df in data.items():\n",
    "            if scale == '1day':\n",
    "                aligned_df = daily_df.reindex(base_df.index, method='ffill')\n",
    "                # 清除跨日数据\n",
    "                mask = (aligned_df.index.time == pd.to_datetime('23:00').time()).reshape(-1, 1)\n",
    "                mask = np.tile(mask, (1, aligned_df.shape[1]))  # 扩展到所有列\n",
    "                \n",
    "                # 应用mask并前向填充\n",
    "                aligned_df = pd.DataFrame(\n",
    "                    np.where(mask, aligned_df.values, np.nan),\n",
    "                    index=aligned_df.index,\n",
    "                    columns=aligned_df.columns\n",
    "                ).ffill()\n",
    "            elif scale in ['4hour','1hour']: \n",
    "                aligned_df = df.resample('30min').interpolate('linear')\n",
    "            else:    \n",
    "                # 前向填充粗粒度数据\n",
    "                aligned_df = df.reindex(base_df.index, method='ffill')\n",
    "            # 在ffill后添加异常过滤\n",
    "            # aligned_df = aligned_df.where(\n",
    "            #     (aligned_df.diff().abs() < 3*aligned_df.std()) | \n",
    "            #     (aligned_df.isna()), \n",
    "            #     method='ffill'\n",
    "            # )\n",
    "            aligned_data[scale] = aligned_df.dropna()\n",
    "        # 统一裁剪时保留有效时间边界\n",
    "        min_len = min(len(df[(df.index >= base_df.index[0]) & \n",
    "                            (df.index <= base_df.index[-1])]) \n",
    "                    for df in aligned_data.values())\n",
    "        for scale in aligned_data:\n",
    "            aligned_data[scale] = aligned_data[scale].iloc[:min_len]\n",
    "\n",
    "        return aligned_data\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\"\"\"\"\"\"  \"\"\"\n",
    "        features = {}\n",
    "        target_idx = idx + self.seq_length\n",
    "\n",
    "        # 获取各尺度特征序列\n",
    "        for scale in self.scaler_factors:\n",
    "            scale_data = self.aligned_data[scale].iloc[idx:idx + self.seq_length]\n",
    "            features[scale] = torch.FloatTensor(scale_data.drop(columns=['close']).values)\n",
    "\n",
    "        # 获取目标值\n",
    "        target = self.aligned_data[self.target_scale]['close'].iloc[\n",
    "                 target_idx:target_idx + self.pred_steps\n",
    "        ]\n",
    "\n",
    "        return features,  torch.FloatTensor(target.values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.aligned_data[self.target_scale]) - self.seq_length - self.pred_steps\n",
    "\n",
    "    def _import_synthetic_data(self, file_path=None, file_name=None):\n",
    "        \"\"\"生成数据\"\"\"\n",
    "        if file_path is None:\n",
    "            return self.data\n",
    "        data = pd.read_csv(f\"{file_path}/{file_name}.csv\",\n",
    "                           parse_dates=['date'],\n",
    "                           index_col='date')\n",
    "\n",
    "        return data.dropna()\n",
    "\n",
    "    def _normalize_data(self):\n",
    "        \"\"\"分离特征与目标的标准化\"\"\"\n",
    "        self.scalers = {'features': {}, 'target': {}}\n",
    "\n",
    "        # 特征标准化（排除close）\n",
    "        for scale in self.scaler_factors:\n",
    "            feature_cols = [c for c in self.aligned_data[scale].columns if c != 'close']\n",
    "\n",
    "            self.aligned_data[scale][feature_cols] = self.feature_scaler.fit_transform(\n",
    "                self.aligned_data[scale][feature_cols]\n",
    "            )\n",
    "            self.scalers['features'][scale] = self.feature_scaler\n",
    "\n",
    "        # 目标值标准化（仅close）\n",
    "        target_scaler = RobustScaler()\n",
    "        for scale in self.scaler_factors:\n",
    "            self.aligned_data[scale]['close'] = target_scaler.fit_transform(\n",
    "                self.aligned_data[scale][['close']]\n",
    "            )\n",
    "        self.scalers['target'] = target_scaler\n",
    "    # 新增反归一化方法\n",
    "    def inverse_transform(self, data, scale_type='target'):\n",
    "        if scale_type == 'target':\n",
    "            return self.scalers['target'].inverse_transform(data)\n",
    "        else:\n",
    "            return self.scalers['features'][scale_type].inverse_transform(data)\n",
    "    def _validate_features(self):\n",
    "        \"\"\"日级数据完整性检查\"\"\"\n",
    "        daily_data = self.aligned_data['1day']\n",
    "        \n",
    "        # 检查每日数据唯一性\n",
    "        day_groups = daily_data.groupby(pd.Grouper(freq='D'))\n",
    "        for date, group in day_groups:\n",
    "            if len(group) > 0:\n",
    "                # 验证当日所有记录相同\n",
    "                if not group.eq(group.iloc[0]).all().all():\n",
    "                    raise ValueError(f\"Daily data inconsistency on {date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "        # 检查时间戳是否在每日23:00\n",
    "        assert all(ts.time() == pd.to_datetime('23:00').time() \n",
    "               for ts in daily_data.resample('D').last().index), \\\n",
    "           \"Daily data timestamps misaligned\"\n",
    "        \n",
    "        \"\"\"确保所有尺度包含相同特征且排除目标列\"\"\"\n",
    "        base_columns = set(self.aligned_data[self.scaler_factors[0]].columns) - {'close'}\n",
    "        for scale in self.scaler_factors:\n",
    "            assert 'close' in self.aligned_data[scale].columns, \\\n",
    "                f\"Close price missing in {scale} scale!\"\n",
    "            assert set(self.aligned_data[scale].columns) - {'close'} == base_columns, \\\n",
    "                f\"Feature mismatch in {scale} scale!\"\n",
    "        for scale in self.scaler_factors:\n",
    "            orig = self.raw_data[scale].close.pct_change().dropna()\n",
    "            aligned = self.aligned_data[scale].close.pct_change().dropna()\n",
    "            assert wasserstein_distance(orig, aligned) < 0.1, f\"{scale}分布偏移超标\"\n",
    "    def print(self):\n",
    "        for scale, data in self.aligned_data.items():\n",
    "            print(f\"{scale}: {data.shape}\")\n",
    "            if(scale == '1day'):\n",
    "                print(data.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiScaleStockDataset(file_path=config['data_path'], \n",
    "                                 scaler_factors=config['scaler_factors'], \n",
    "                                 prefix='train-')\n",
    "# dataset.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_temporal_alignment(dataset, n_samples=3):\n",
    "    time_serial = dataset.time_serial\n",
    "    fig, axs = plt.subplots(n_samples, 1, figsize=(15, 3*n_samples))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        idx = i * dataset.seq_length  # Calculate actual data position\n",
    "        sample_dates = time_serial[idx:idx+dataset.seq_length]\n",
    "        sample = dataset[i]\n",
    "        for scale in dataset.scaler_factors:\n",
    "            features = dataset.aligned_data[scale].iloc[idx:idx+dataset.seq_length, 0]\n",
    "            axs[i].plot(sample_dates, features, label=f'{scale} close')\n",
    "        axs[i].set_title(f'Sample {i} - Feature Alignment')\n",
    "        plt.setp(axs[i].xaxis.get_majorticklabels(), rotation=45)\n",
    "        axs[i].legend()\n",
    "        axs[i].xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        axs[i].xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d %H:%M\"))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_temporal_alignment(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 2. 训练日志系统\n",
    "# ====================\n",
    "class TrainingLogger:\n",
    "    def __init__(self, log_dir=\"logs\"):\n",
    "        self.metrics = {\n",
    "            'train_loss': [],\n",
    "            'train/batch_loss':[],\n",
    "            'val_loss': [],\n",
    "            'lr': [],\n",
    "            'learning_rate/group_0': [],  # 显式初始化参数组0\n",
    "            'learning_rate/group_1': []   # 根据实际参数组数量添加\n",
    "        }\n",
    "        # TensorBoard日志\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.writer = SummaryWriter(f'{log_dir}/tensorboard/{timestamp}')\n",
    "        \n",
    "        # 文本日志\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(f'{log_dir}/training.log',encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger('PyTorch Training')\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        \"\"\"记录标量数据\"\"\"\n",
    "        self.writer.add_scalar(tag, value, step)\n",
    "        self.metrics[tag].append((step, value))\n",
    "\n",
    "    def log_histogram(self, tag, values, step):\n",
    "        \"\"\"记录参数分布\"\"\"\n",
    "        self.writer.add_histogram(tag, values, step)\n",
    "\n",
    "    def log_learning_rate(self, optimizer, step):\n",
    "        \"\"\"记录学习率\"\"\"\n",
    "        for i, group in enumerate(optimizer.param_groups):\n",
    "            self.log_scalar(f'learning_rate/group_{i}', group['lr'], step)\n",
    "\n",
    "    def log_gradients(self, model, step):\n",
    "        \"\"\"记录梯度分布\"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                self.log_histogram(f\"gradients/{name}\", param.grad, step)\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "class AdvancedLogger(TrainingLogger):\n",
    "    def log_model_graph(self, model, dummy_input):\n",
    "        \"\"\"记录模型计算图\"\"\"\n",
    "        self.writer.add_graph(model, dummy_input)\n",
    "    \n",
    "    def log_confusion_matrix(self, y_true, y_pred, classes, step):\n",
    "        \"\"\"记录混淆矩阵\"\"\"\n",
    "        # cm = confusion_matrix(y_true, y_pred)\n",
    "        # fig = plt.figure()\n",
    "        # sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "        # self.writer.add_figure('confusion_matrix', fig, step)\n",
    "    \n",
    "    def log_embeddings(self, embeddings, metadata, step):\n",
    "        \"\"\"记录嵌入向量\"\"\"\n",
    "        self.writer.add_embedding(\n",
    "            embeddings,\n",
    "            metadata=metadata,\n",
    "            global_step=step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleAwareExpert(nn.Module):    \n",
    "    \"\"\"专家\"\"\"\n",
    "    def __init__(self, input_dim, scale_type, arch_config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 设置默认值\n",
    "        self.arch_config = {\n",
    "            'high_freq': {\n",
    "                'conv_channels': [input_dim, 32, 64],\n",
    "                'lstm_units': 64,\n",
    "                'bidirectional': True\n",
    "            },\n",
    "            'low_freq': {\n",
    "                'transformer_layers': 2,\n",
    "                'nhead': 4,\n",
    "                'ff_dim': 128\n",
    "            }\n",
    "        }\n",
    "        # 用配置覆盖默认值\n",
    "        self.arch_config.update(arch_config)\n",
    "        self.scale_type = scale_type\n",
    "\n",
    "        self.build_model(input_dim)\n",
    "    \n",
    "    def build_model(self, input_dim):\n",
    "        \"\"\"构建模型\"\"\"\n",
    "        if '30min' in self.scale_type or '1hour' in self.scale_type:\n",
    "            \"\"\"高频专家: CNN + BiLSTM \"\"\"\n",
    "            high_freq = self.arch_config.get('high_freq')\n",
    "            self.proj_in = nn.Linear(input_dim, 30)  # 新增维度对齐层\n",
    "            self.conv = nn.Conv1d(input_dim, \n",
    "                                  out_channels=16,\n",
    "                                  kernel_size=3, \n",
    "                                  padding='same')\n",
    "            self.lstm = nn.LSTM(16, # 输入维度\n",
    "                                high_freq.get('lstm_units',32),  #隐藏单元数\n",
    "                                num_layers=high_freq.get('num_layers',1),  # 堆叠层数，增加模型复杂度以捕捉多级时序模式\n",
    "                                batch_first=True, \n",
    "                                dropout=0.2, # 随机丢弃部分神经元，减少过拟合\n",
    "                                bidirectional=True # 双向LSTM，增强模型表达能力， 输出时 hidden_size * 2\n",
    "                                )\n",
    "            self.proj = nn.Linear(64, 32)\n",
    "        else:\n",
    "            \"\"\"低频专家: LSTM \"\"\"\n",
    "            self.proj_in = nn.Linear(input_dim, 4)  # 3维→4维\n",
    "            self.encoder_layer = nn.TransformerEncoderLayer(d_model=4, \n",
    "                                                            nhead=2, \n",
    "                                                            dim_feedforward=64, \n",
    "                                                            batch_first=True)\n",
    "            self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n",
    "            self.proj = nn.Linear(4, 32)\n",
    "        self.predictor = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"输入: [batch, seq_len, dim]\"\"\"\n",
    "        if '30min' in self.scale_type or '1h' in self.scale_type:\n",
    "            x = self.proj_in(x)\n",
    "            x = self.conv(x.permute(0,2,1)).permute(0,2,1)\n",
    "            x, _ = self.lstm(x)\n",
    "            x = x[:, -1, :]  # 取最后一个时间步 [batch, 4]\n",
    "            x = self.proj(x)\n",
    "        else:\n",
    "            x = self.proj_in(x)\n",
    "            x = self.transformer(x)\n",
    "            x = self.proj(x[:,-1,:]) # 只取最后一个时间步的输出\n",
    "        return self.predictor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DymaMoE(nn.Module):\n",
    "    \"\"\"动态混合专家网络\"\"\"\n",
    "    def __init__(self, input_dim=30, experts=['30min','1hour','4hour','1day'], hidden_dim=64,model_config=None, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleDict({\n",
    "            scale: ScaleAwareExpert(input_dim, scale, arch_config=model_config.get('expert_config')) for scale in experts\n",
    "        })\n",
    "        gate_network = model_config.get('gate_network')\n",
    "        \"\"\"动态构建门控网络\"\"\"\n",
    "        layers = []\n",
    "        prev_dim = input_dim * len(experts)\n",
    "        for dim in gate_network.get('hidden_dims'):\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(gate_network.get('dropout_rate')))\n",
    "            prev_dim = dim\n",
    "        layers.append(nn.Linear(prev_dim, len(self.experts)))\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "        self.gate = nn.Sequential(*layers)\n",
    "        # self.gate = nn.Sequential(\n",
    "        #     nn.Linear(input_dim * len(experts), hidden_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(gate_network.get('dropout_rate')),\n",
    "        #     nn.Linear(hidden_dim, len(experts)),\n",
    "        #     nn.Softmax(dim=-1)\n",
    "        # )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        expert_outputs = {}\n",
    "        for scale, expert in self.experts.items():\n",
    "            expert_outputs[scale] = expert(inputs[scale]).squeeze(-1)  # [32,1] -> [32]\n",
    "        # 动态门控\n",
    "        gate_input = torch.cat([v for v in inputs.values()], dim=-1)\n",
    "    \n",
    "        weights = self.gate(gate_input.mean(dim=1))  # (batch, num_experts)\n",
    "        \n",
    "        # 加权融合\n",
    "        combined = sum(weights[:, i] * expert_outputs[scale] \n",
    "                      for i, scale in enumerate(self.experts.keys()))\n",
    "        return combined.unsqueeze(-1), expert_outputs  \n",
    "    def print_model_structure(self):\n",
    "        \"\"\"打印各专家结构\"\"\"\n",
    "        for name, expert in self.experts.items():\n",
    "            print(f\"\\n=== {name}专家架构 ===\")\n",
    "            print(\"卷积通道:\", expert.arch_config['high_freq']['conv_channels'])\n",
    "            print(\"LSTM单元:\", expert.arch_config['high_freq']['lstm_units'])\n",
    "            print(\"双向结构:\", expert.arch_config['high_freq']['bidirectional'])\n",
    "            print(\"实际参数量:\", sum(p.numel() for p in expert.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAnalyzer:\n",
    "    def __init__(self, model, input_size):\n",
    "        self.model = model\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def print_summary(self):\n",
    "        \"\"\"打印类似Keras的模型摘要\"\"\"\n",
    "        try:\n",
    "            print(\"\\nModel Architecture Summary:\")\n",
    "            summary(self.model, input_size=self.input_size, \n",
    "                    depth=3, \n",
    "                    col_names=[\"input_size\", \"output_size\", \"num_params\"])\n",
    "        except ImportError:\n",
    "            self._fallback_summary()\n",
    "    \n",
    "    def _fallback_summary(self):\n",
    "        \"\"\"备选简易摘要\"\"\"\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        total_params = 0\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"{name:30} | Shape: {str(param.shape):20} | Params: {param.numel()}\")\n",
    "                total_params += param.numel()\n",
    "        print(f\"\\nTotal Trainable Parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model_config =None, trade_config = None):\n",
    "    \"\"\"train dataset\"\"\"\n",
    "    logger = TrainingLogger(\"../logs\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DymaMoE(input_dim=30, model_config=model_config).to(device)\n",
    "\n",
    "    model.print_model_structure()\n",
    "    # 使用示例\n",
    "    test_input = {\n",
    "        '30min': torch.randn(32, 24, 30),\n",
    "        '1hour': torch.randn(32, 24, 30),\n",
    "        '4hour': torch.randn(32, 24, 4),\n",
    "        '1day': torch.randn(32, 24, 1)\n",
    "    }\n",
    "    # analyzer = ModelAnalyzer(model, input_size=[(24,30), (24,30), (24,4), (24,1)])  # seq_len=24, input_dim=30\n",
    "    # analyzer.print_summary()\n",
    "    optimizer = torch.optim.AdamW([\n",
    "         {'params': model.experts.parameters(), 'lr': model_config['learning_rate']},\n",
    "        {'params': model.gate.parameters(), 'lr': 1e-3}\n",
    "    ])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    def hybrid_loss(pred, target, expert_outs, alpha=0.7):\n",
    "        pred = pred.squeeze(-1)  # (32,1) → (32)\n",
    "        target = target.squeeze()  # [32] remains\n",
    "        mse = nn.MSELoss()\n",
    "        main_loss = mse(pred, target)\n",
    "        \n",
    "        # 趋势一致性约束\n",
    "        trends = torch.stack([torch.sign(out.detach()) for out in expert_outs.values()])\n",
    "        consistency = torch.mean(torch.prod(trends, dim=0))\n",
    "        \n",
    "        return alpha*main_loss + (1-alpha)*(1 - consistency)\n",
    "\n",
    "    loss_history = []\n",
    "    expert_weights = []\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    loader = DataLoader(dataset, batch_size=trade_config['batch_size'], shuffle=True)\n",
    "    epochs = trade_config['epochs']\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            running_loss = 0.0\n",
    "            model.train()\n",
    "            # 学习率记录\n",
    "            logger.log_learning_rate(optimizer, epoch)\n",
    "            for batch_idx, (inputs, target) in enumerate(loader):\n",
    "            # 修复输入格式问题\n",
    "                if isinstance(inputs, dict):  # 保持原有字典处理\n",
    "                    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "                else:  # 处理张量输入的情况\n",
    "                    inputs = inputs.to(device)\n",
    "                \n",
    "                target = target.to(device)\n",
    "                \n",
    "                pred, expert_outs = model(inputs)\n",
    "                loss = hybrid_loss(pred, target, expert_outs)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "                # 梯度记录\n",
    "                if batch_idx % 10 == 0:\n",
    "                    logger.log_gradients(model, epoch * len(loader) + batch_idx)\n",
    "                    \n",
    "                optimizer.step()\n",
    "                # 记录损失\n",
    "                running_loss += loss.item()\n",
    "                if batch_idx % 50 == 49:\n",
    "                    avg_loss = running_loss / 50\n",
    "                    global_step = epoch * len(loader) + batch_idx\n",
    "                    logger.log_scalar('train/batch_loss', avg_loss, global_step)\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "                    # 参数分布记录\n",
    "                    for name, param in model.named_parameters():\n",
    "                        logger.log_histogram(f\"parameters/{name}\", param, global_step)\n",
    "                # 收集预测结果\n",
    "                with torch.no_grad():\n",
    "                    preds = dataset.inverse_transform(pred.cpu().numpy())\n",
    "                    targets_orig = dataset.inverse_transform(target.cpu().numpy())\n",
    "                    predictions.extend(preds)\n",
    "                    targets.extend(targets_orig)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            # 记录专家权重\n",
    "            with torch.no_grad():\n",
    "                dummy_input = {k: torch.randn(1,24,30).to(device) for k in model.experts}\n",
    "                _, weights = model(dummy_input)\n",
    "                expert_weights.append([list(out.parameters())[0].detach().cpu().numpy().mean() for out in model.experts.values()])\n",
    "            avg_loss = epoch_loss / len(loader)\n",
    "            loss_history.append(avg_loss)    \n",
    "            scheduler.step()\n",
    "            # 控制台日志\n",
    "            logger.logger.info(\n",
    "                f\"Epoch {epoch+1}/{epochs} | \"\n",
    "                f\"Train Loss: {loss.item():.4f} | \"\n",
    "                # f\"Val Loss: {val_loss:.4f} | \"\n",
    "                f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            )\n",
    "    finally:\n",
    "        logger.close()\n",
    "    return model, (loss_history, expert_weights), (targets, predictions)\n",
    "\n",
    "model, history, results = train(dataset=dataset, \n",
    "                                model_config=config['props'][0]['model_config'], \n",
    "                                trade_config = config['props'][0]['trade_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(loss_history, expert_weights, targets, predictions):\n",
    "    \"\"\"训练过程可视化分析\"\"\"\n",
    "    plt.figure(figsize=(15,5))\n",
    "    smooth_loss = pd.Series(loss_history).rolling(5, min_periods=1).mean()\n",
    "    # 损失曲线\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(smooth_loss, label='Training Loss')\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\"), plt.ylabel(\"Loss\")\n",
    "    \n",
    "    # 专家权重分布\n",
    "    plt.subplot(1,3,2)\n",
    "    sns.heatmap(np.array(expert_weights), cmap='viridis', \n",
    "                xticklabels=['30min','1hour','4hour','1day'])\n",
    "    plt.title(\"Expert Weights Distribution\")\n",
    "    plt.xlabel(\"Experts\"), plt.ylabel(\"Epoch\")\n",
    "    \n",
    "    # 预测结果示例\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.scatter(targets[:100], predictions[:100], alpha=0.5)\n",
    "    # plt.plot([min(targets), max(targets)], [min(targets), max(targets)], 'r--')\n",
    "    plt.plot(targets[:200], label='True Price')\n",
    "    plt.plot(predictions[:200], alpha=0.7, label='Predicted')\n",
    "    plt.title(\"Predictions vs Ground Truth\")\n",
    "    plt.xlabel(\"True Values\"), plt.ylabel(\"Predictions\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"可视化分析\"\"\"\n",
    "visualize_training(history[0], history[1], targets=results[0], predictions=results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(targets, predictions, config):\n",
    "    # 确保使用反归一化后的原始价格单位\n",
    "    y_true = np.array(targets)\n",
    "    y_pred = np.array(predictions)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    metric_funcs = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'R2': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': np.mean(np.abs((y_true - y_pred)/y_true)) * 100,\n",
    "        'Sharpe': (np.mean(y_pred) - np.mean(y_true)) / np.std(y_true - y_pred)\n",
    "    }\n",
    "    metrics = {}\n",
    "    # 根据配置筛选指标\n",
    "    for metric in config['metrics']:\n",
    "        if metric in metric_funcs:\n",
    "            metrics[metric] = metric_funcs[metric]\n",
    "    return metrics\n",
    "def visualize_training2(dataset, loss_history, expert_weights, targets, predictions, config):\n",
    "    vis_config = config.get('visualization')\n",
    "    plt.figure(figsize=(20,6))\n",
    "    \n",
    "    # 1. 损失曲线（增加指标显示）\n",
    "    plt.subplot(1,4,1)\n",
    "    smooth_loss = pd.Series(loss_history).rolling(5, min_periods=1).mean()\n",
    "    plt.plot(smooth_loss, label='Smoothed Loss')\n",
    "    \n",
    "    # 计算并显示关键指标\n",
    "    metrics = calculate_metrics(targets, predictions, config)\n",
    "    metric_text = \"\\n\".join([f\"{k}: {v:.4f}\" for k,v in metrics.items()])\n",
    "    plt.text(0.5, 0.3, metric_text, transform=plt.gca().transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.5))\n",
    "    \n",
    "    plt.title(\"Training Loss & Metrics\")\n",
    "    plt.xlabel(\"Epoch\"), plt.ylabel(\"Loss\")\n",
    "\n",
    "    # 2. 专家权重演化\n",
    "    plt.subplot(1,4,2)\n",
    "    weights_df = pd.DataFrame(expert_weights, \n",
    "                            columns=['30min','1h','4h','1d'])\n",
    "    sns.lineplot(data=weights_df, dashes=False)\n",
    "    plt.title(\"Expert Weights Evolution\")\n",
    "    plt.xlabel(\"Epoch\"), plt.ylabel(\"Weight Value\")\n",
    "\n",
    "    # 3. 价格预测对比\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.plot(targets[:200], label='True Price')\n",
    "    plt.plot(predictions[:200], alpha=0.7, label='Predicted')\n",
    "    \n",
    "    # 添加残差分布子图\n",
    "    ax = plt.gca()\n",
    "    ax.inset_axes([0.6,0.15,0.35,0.25]).hist(\n",
    "        np.array(targets)-np.array(predictions),\n",
    "        bins=30, density=True\n",
    "    )\n",
    "    plt.title(\"Residual Distribution\")\n",
    "    \n",
    "    plt.title(\"Price Prediction Comparison\")\n",
    "    plt.xlabel(\"Time Step\"), plt.ylabel(\"Price (RMB)\")\n",
    "    plt.legend()\n",
    "\n",
    "    # 4. 特征重要性热图（新增）\n",
    "    plt.subplot(1,4,4)\n",
    "    feature_importance = dataset.aligned_data['30min'].corrwith(\n",
    "        pd.Series(predictions[:len(dataset.aligned_data['30min'])])\n",
    "    ).sort_values()\n",
    "    sns.heatmap(feature_importance.to_frame().T, cmap='coolwarm',\n",
    "                annot=True, fmt=\".2f\")\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "visualize_training2(dataset, *history, targets=results[0], predictions=results[1], config=config.get('props')[0].get('eval_config'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
